# -*- coding: utf-8 -*-
"""ECE228project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kFb6Ez2zsPTA6nxmH_UbG0P0aMf2Qgks

**bold text**
# Prepartion
"""

from google.colab import drive
drive.mount('/content/drive')

cd drive/My Drive

cd Colab\ Notebooks

!pip install keras==2.2.0
!pip install tensorflow==1.14.0

import tensorflow as tf
print(tf.__version__)

import numpy as np
import pandas as pd
import seaborn as sns; sns.set()
import matplotlib.pyplot as plt
import cv2
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score 
from sklearn.metrics import log_loss
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense,Activation,Dropout
from keras.callbacks import ModelCheckpoint
from sklearn.model_selection import StratifiedShuffleSplit
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

#read data
trainO = pd.read_csv('train.csv')
testO = pd.read_csv('test.csv')

(trainO.head())

#sample features of data
trainO.iloc[0:5,[0,1,2,3,66,67,130,131]]

#clean data
le = LabelEncoder().fit(trainO.species) 
labels = le.transform(trainO.species)  
classes = list(le.classes_)  
test_ids = testO.id  
train = trainO.drop(['id', 'species'], axis=1)
test = testO.drop(['id'], axis=1)

#split original train data into validation & train data sets in 8:2 propotion
split_data = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)
split_data.get_n_splits(train, labels)
for train_index, val_index in split_data.split(train, labels):   
    X_train, X_val = train.values[train_index], train.values[val_index]
    y_train, y_val = labels[train_index], labels[val_index]

# visulize sample leaves
def visualize(ID):  
    imgn = str(ID) + '.jpg'
    limg = plt.imread('images/'+imgn)
    w = limg.shape[1]
    h = limg.shape[0]
    tg = np.zeros((160, 160), np.uint8)
    if w < h:
        sh = 160
        sw = int((float(sh)/h)*w)
        scale = cv2.resize(limg, (sw,sh), interpolation = cv2.INTER_AREA)
        cl = (160-sw)/2
        tg[:, int(cl):int(cl)+int(sw)] = scale    
    else:      
        sw = 160
        sh = int((float(sw)/w)*h)
        scale = cv2.resize(limg,(sw,sh),interpolation=cv2.INTER_AREA)
        cl = (160-sh)/2
        tg[int(cl):int(cl+sh),:] = scale 
    
    return tg

for x in range (1,3):
  plt.imshow(visualize(x)); 
  plt.title(f'leaf {x}')
  plt.show()

#visualize leaves of given species
def visualize_same(species):
    ans = 240 * np.ones([160, 160*2], np.uint8)  
    if type(species) ==int or type(species)==np.int64:
        im_id = np.where(labels==species)[0]
        labelx = str(species) + ' ' + train_raw.species[im_id[0]]
    else:
      print ('Please Input Index of species')
    i = 0
    for image_im_id in im_id:
        trueid = train_raw.id[image_im_id]
        imag = visualize(trueid)
        ans = np.append(ans, imag, axis=1)

    cv2.putText(ans, f'species:{labelx}', (0,150), cv2.FONT_HERSHEY_COMPLEX, 0.7, (96,2,10), 2)         
    return ans, labelx

species_img, labelx = visualize_same(2) 
fig = plt.figure(num=None, figsize=(17, 17),dpi=1500)
plt.title(f'{labelx}')
plt.imshow(species_img) 
plt.show()

# visulize the first two wrong model results in contrast
def error_set(pred, y_val):
    error_in = np.where(pred != y_val)[0];
    error_sets = y_val[error_in]
    should_be = pred[error_in]
    print ('example contrast results:')
    if (len(error_sets) <= 2):
      for i in range(len(error_sets)):
        print('wrong pred')
        species_img, labelx = visualize_same(error_sets[i]) 
        fig = plt.figure(num=None, figsize=(17, 17),dpi=1500)
        plt.title(f'species_{error_sets[i]}')
        plt.imshow(species_img) 
        plt.show()
        print('should be')
        species_img, labelx = visualize_same(should_be[i]) 
        fig = plt.figure(num=None, figsize=(17, 17),dpi=1500)
        plt.title(f'species_{should_be[i]}')
        plt.imshow(species_img) 
        plt.show()
    else:
      for i in range(2):
        print('wrong pred')
        species_img, labelx = visualize_same(error_sets[i]) 
        fig = plt.figure(num=None, figsize=(17, 17),dpi=1500)
        plt.title(f'species_{error_sets[i]}')
        plt.imshow(species_img) 
        plt.show()
        print('should be')
        species_img, labelx = visualize_same(should_be[i]) 
        fig = plt.figure(num=None, figsize=(17, 17),dpi=1500)
        plt.title(f'species_{should_be[i]}')
        plt.imshow(species_img) 
        plt.show()
    return error_sets

"""# Naive Bayes"""

pt = pd.DataFrame(columns = ['Classifier', 'accuracy', 'loss'])

gus = GaussianNB().fit(X_train, y_train)
gus_pred = gus.predict(X_val)
nameg = gus.__class__.__name__

gus_loss = gus.predict_proba(X_val)
print(f'accuracy for {nameg}: {100*accuracy_score(y_val, gus_pred)}%')
print(f'loss: {log_loss(y_val, gus_loss)}')

pt=pt[0:0]
gu = pd.DataFrame([[nameg,55.05,15.50]],columns = ['Classifier', 'accuracy', 'loss'])
pt = pt.append(gu)

pt

es = error_set(gus_pred,y_val)

cm_nb = confusion_matrix(y_val, gus_pred)
plt.title("Naive bayes")
sns.heatmap(cm_nb,cbar = False)
plt.show()

"""# SVC"""

svc = SVC(kernel="rbf",probability=True,C=0.025)

svcp = svc.fit(X_train, y_train)
svc_pred = svcp.predict(X_val)
names = svc.__class__.__name__
svc_loss = svc.predict_proba(X_val)
print(f'accuracy for {names}: {100*accuracy_score(y_val, svc_pred)}%')
print(f'loss: {log_loss(y_val, svc_loss)}')

su = pd.DataFrame([[names,84.34,4.64]],columns = ['Classifier', 'accuracy', 'loss'])
pt = pt.append(su)

es = error_set(svc_pred,y_val)

cm_svc = confusion_matrix(y_val, svc_pred)
plt.title("SVC")
sns.heatmap(cm_svc,cbar = False)
plt.show()

"""# Logistic"""

clf = LogisticRegression(solver='newton-cg', multi_class='multinomial')
clfp = clf.fit(X_train, y_train)
clf_pred = clfp.predict(X_val)
names = clf.__class__.__name__
clf_loss = clf.predict_proba(X_val)
print(f'accuracy for {names}: {100*accuracy_score(y_val, clf_pred)}%')
print(f'loss: {log_loss(y_val, clf_loss)}')

su = pd.DataFrame([[names,65.65,4.16]],columns = ['Classifier', 'accuracy', 'loss'])
pt =pt.append(su)

es = error_set(clf_pred,y_val)

cm_logistic = confusion_matrix(y_val, clf_pred)
plt.title("Logistic")
sns.heatmap(cm_logistic,cbar = False)
plt.show()

"""# KNN"""

clf = KNeighborsClassifier(1)
clfp = clf.fit(X_train, y_train)
clf_pred = clfp.predict(X_val)
names = clf.__class__.__name__
clf_loss = clf.predict_proba(X_val)
print(f'accuracy for {names}: {100*accuracy_score(y_val, clf_pred)}%')
print(f'loss: {log_loss(y_val, clf_loss)}')

su = pd.DataFrame([[names,91.91,2.79]],columns = ['Classifier', 'accuracy', 'loss'])
pt=pt.append(su)

es = error_set(clf_pred,y_val)

cm_KNN = confusion_matrix(y_val, clf_pred)
plt.title("KNN")
sns.heatmap(cm_KNN,cbar = False)
plt.show()

"""# Linear Discriminant"""

clf = LinearDiscriminantAnalysis()
clfp = clf.fit(X_train, y_train)
clf_pred = clfp.predict(X_val)
names = clf.__class__.__name__
clf_loss = clf.predict_proba(X_val)
print(f'accuracy for {names}: {100*accuracy_score(y_val, clf_pred)}%')
print(f'loss: {log_loss(y_val, clf_loss)}')

su = pd.DataFrame([[names,97.47,0.51]],columns = ['Classifier', 'accuracy', 'loss'])
pt=pt.append(su)

es = error_set(clf_pred,y_val)

cm_Linear_Discriminant = confusion_matrix(y_val, clf_pred)
plt.title("Linear_Discriminant")
sns.heatmap(cm_Linear_Discriminant,cbar = False)
plt.show()

"""# CNN"""

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

train_id = train['id']
train_species = train['species']
train = train.drop(['id', 'species'], axis = 1)
train = StandardScaler().fit(train).transform(train)

test_id = test['id']
test=test.drop(['id'],axis=1)
test=StandardScaler().fit(test).transform(test)

LE = LabelEncoder().fit(train_species)
labels = LE.transform(train_species)

splitdata = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)
splitdata.get_n_splits(train, labels)
for train_index, valid_index in splitdata.split(train,labels):
  X_train, X_valid = train[train_index], train[valid_index]
  y_train, y_valid = labels[train_index], labels[valid_index]

print(X_train.shape, X_valid.shape)

print(y_train.shape, y_valid.shape)

y_train1 = to_categorical(y_train)
y_valid1 = to_categorical(y_valid)

print(y_train.shape, y_valid.shape)

def CNN():
  model = Sequential()
  model.add(Dense(800,activation='relu',input_shape=(192,)))
  model.add(Dropout(0.3))
  model.add(Dense(400,activation='relu'))
  model.add(Dropout(0.3))
  model.add(Dense(200,activation='relu'))
  model.add(Dropout(0.3))
  model.add(Dense(99,activation='softmax'))
  return model

CNNmodel = CNN()
CNNmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history_CNN=CNNmodel.fit(X_train,y_train1,validation_data=(X_valid, y_valid1),epochs=250,batch_size=150)

print(history_CNN.history.keys())

plt.figure(figsize=(8,8))
plt.subplot(221)
plt.plot(history_CNN.history['acc'])
plt.plot(history_CNN.history['val_acc'])
plt.title('acc vs. epoch', fontsize = 12)
plt.xlabel('epoch', fontsize = 12)
plt.ylabel('acc',fontsize=12)
plt.legend(('train','validation'),fontsize = 12, loc = 4)

plt.subplot(222)
plt.plot(history_CNN.history['loss'])
plt.plot(history_CNN.history['val_loss'])
plt.title('\nloss vs. epoch', fontsize = 12)
plt.xlabel('epoch', fontsize = 12)
plt.ylabel('loss',fontsize=12)
plt.legend(('train','validation'),fontsize = 12, loc = 1)

plt.subplot(212)
y_pred = CNNmodel.predict_classes(X_valid)
cm_CNN = confusion_matrix(y_valid, y_pred)
plt.title("\nCNN confusion matrix")
sns.heatmap(cm_CNN,cbar = False)
plt.show()



y_pred = CNNmodel.predict_classes(X_valid)
cm_CNN = confusion_matrix(y_valid, y_pred)
plt.title("CNN")
sns.heatmap(cm_CNN,cbar = False)
plt.show()

su = pd.DataFrame([['CNN',98.99,0.0353]],columns = ['Classifier', 'accuracy', 'loss'])
pt=pt.append(su)

pt

plt.subplot(221)
sns.barplot(x='accuracy',y='Classifier',data=pt)
plt.xlabel("Accuracy(%)")
plt.title('Accuracy for classifiers')
plt.subplot(222)
sns.barplot(x='loss',y='Classifier',data=pt)
plt.xlabel("loss(%)")
plt.title('loss for classifiers')
plt.show()